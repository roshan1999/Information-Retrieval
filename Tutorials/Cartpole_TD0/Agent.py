import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as func

torch.manual_seed(0)


class Network(nn.Module):
    def __init__(self, observation_space, action_space, layer1_size, layer2_size, lr):
        super(Network, self).__init__()
        self.fc1 = nn.Linear(observation_space, layer1_size)
        self.fc2 = nn.Linear(layer1_size, layer2_size)
        self.fc3 = nn.Linear(layer2_size, action_space)
        self.optimizer = optim.Adam(self.parameters(), lr=lr)

    def forward(self, observation):
        state = torch.Tensor(observation)
        network_output = self.fc3(func.relu(self.fc2(func.relu(self.fc1(state)))))
        return network_output


class Agent:
    def __init__(self, actor_lr, critic_lr, gamma, observation_space, action_space, layer1_size, layer2_size):
        self.gamma = gamma  # Used for computing TD error later

        self.log_probs = None  # Used to store the log_prob which is later used to update theta

        # Initialize the actor network's object
        self.actor = Network(observation_space=observation_space,
                             action_space=action_space,
                             layer1_size=layer1_size,
                             layer2_size=layer2_size,
                             lr=actor_lr)

        # Initialize the critic network's object
        self.critic = Network(observation_space=observation_space,
                              layer1_size=layer1_size,
                              layer2_size=layer2_size,
                              action_space=1,
                              lr=critic_lr)

    # Function to select an action based on policy mu_theta
    def select_action(self, state):
        state = torch.tensor(state).float()

        # Feed-forward state through neural network to get state representation parameterized by theta
        actor_state = self.actor.forward(state)

        # Performing softmax to get probabilities
        soft_probs = func.softmax(actor_state, dim=-1)

        # Taking categorical distribution
        actor_probs = torch.distributions.Categorical(soft_probs)

        # Choosing an action based on the probabilities generated by categorical distribution
        action = actor_probs.sample()

        # Saving the log probabilities -- gradient of this would be used to update the theta parameters later
        self.log_probs = actor_probs.log_prob(action)

        # Returning the index of the action taken.
        return action.item()

    # Function to perform parameter updates and calculate the critic values.
    def update(self, curr_state, next_state, reward, done):
        # Resetting the gradients of the actor and critic network
        self.actor.optimizer.zero_grad()
        self.critic.optimizer.zero_grad()

        # Converting the current and the next state to tensors.
        state = torch.tensor(curr_state).float()
        state_ = torch.tensor(next_state).float()

        # Calculating the critic value from the neural network.
        # The neural network outputs a single value as the critic value
        # This can be used directly to calculate TD error
        critic_value = self.critic.forward(state)
        critic_value_ = self.critic.forward(state_)

        # Calculating TD error
        # Done flag used to indicate that the final state is achieved
        # Hence the critic_value_ doesn't exist
        delta = reward + self.gamma * (1 - int(done)) * critic_value_ - critic_value

        # Calculating the value loss i.e. change in w (parameter for critic)
        value_loss = delta ** 2

        # Calculating the policy loss i.e. change in theta (parameter for actor)
        policy_loss = -self.log_probs * delta

        # Calculating the total loss to backpropagate the loss to the networks.
        total_loss = policy_loss + value_loss

        # Backpropagate the loss
        total_loss.backward()

        # Update the parameters using adam optimizer.
        self.actor.optimizer.step()
        self.critic.optimizer.step()
